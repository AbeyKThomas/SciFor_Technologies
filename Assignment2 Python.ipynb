{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3455f4ab",
   "metadata": {},
   "source": [
    "### Python Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b0ee9",
   "metadata": {},
   "source": [
    "**1.What is the difference between a list and a tuple in Python?**\n",
    "\n",
    "Ans : \n",
    "Both \"list\" and \"tuple\" are fundamental datatypes in python . The key difference between tuples and lists is that while tuples are immutable objects, lists are mutable. This means tuples cannot be changed while lists can be modified.\n",
    "\n",
    "Lists are created using square brackets [ ].\n",
    "\n",
    "\n",
    "   Eg:\n",
    "         list1 = [1, 2, 3,4]\n",
    "         \n",
    "         \n",
    "         \n",
    "Tuples are created using parentheses ( ).\n",
    "\n",
    "\n",
    "   Eg:\n",
    "         tuple1 = (1, 2, 3, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258a9a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Eg: \n",
    "list1 = [1, 2, 3,4]\n",
    "\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb247d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Eg: \n",
    "tuple1 = (1, 2, 3, 4)\n",
    "tuple1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bfa9b8",
   "metadata": {},
   "source": [
    "**2.How can you iterate through a list in Python?**\n",
    "\n",
    "Ans: \n",
    "Ways to Iterate over a List in Python\n",
    "\n",
    "i.Using a For Loop:\n",
    "\n",
    "      The most common method is using a for loop. This loop iterates over each element in      the list.\n",
    "        \n",
    "ii. Using a While Loop:\n",
    "\n",
    "      You can use a while loop along with an index to iterate through the list.\n",
    "        \n",
    "iii. Using Enumerate:\n",
    "\n",
    "        The enumerate function can be used to iterate through both the elements and their indices.\n",
    "        \n",
    "iv. Using List Comprehension:\n",
    "\n",
    "      List comprehension is a concise way to iterate through a list and perform an operation on each element.\n",
    "        \n",
    "v. Using Iter():\n",
    "\n",
    "      The iter() function can be used to create an iterator for the list, and the next() function can be used       to retrieve each item.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b48a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Examples:\n",
    "#Using a For Loop    \n",
    "listA = [1, 2, 3, 4, 5]\n",
    "for item in listA:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79994433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Using a While Loop\n",
    "\n",
    "listA = [1, 2, 3, 4, 5]\n",
    "index = 0\n",
    "\n",
    "while index < len(listA):\n",
    "    print(listA[index])\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df51fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Value: 1\n",
      "Index: 1, Value: 2\n",
      "Index: 2, Value: 3\n",
      "Index: 3, Value: 4\n",
      "Index: 4, Value: 5\n"
     ]
    }
   ],
   "source": [
    "#Using Enumerate\n",
    "\n",
    "listA = [1, 2, 3, 4, 5]\n",
    "\n",
    "for index, item in enumerate(listA):\n",
    "    print(f\"Index: {index}, Value: {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4936e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using List Comprehension\n",
    "\n",
    "listA = [1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "[print(item * 2) for item in listA]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3442e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Using Iter()\n",
    "\n",
    "listA = [1, 2, 3, 4, 5]\n",
    "iterA = iter(listA)\n",
    "\n",
    "for item in iterA:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d033e7a",
   "metadata": {},
   "source": [
    "**3.How do you handle exceptions in Python?**\n",
    "\n",
    "Ans : \n",
    "\n",
    "Different types of exceptions in python:\n",
    "\n",
    "a. Try and Except:\n",
    "\n",
    "      The try block is used to enclose the code that might raise an exception. If an exception occurs, the     code in the except block is executed.\n",
    "      You can have multiple except blocks to handle different types of exceptions.\n",
    "      \n",
    "b. Else:\n",
    "\n",
    "      The else block is optional and is executed if no exceptions are raised in the try block.\n",
    "      \n",
    "c. Finally:\n",
    "\n",
    "      The finally block is optional and is always executed whether an exception is raised or not. It's often used for cleanup operations.\n",
    "      \n",
    "d. Raising Exceptions:\n",
    "\n",
    "      You can use the raise statement to explicitly raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d84786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division by zero is not allowed.\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "#Try and Except:\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    result = 10 / 0\n",
    "except ZeroDivisionError:\n",
    "    \n",
    "    print(\"Division by zero is not allowed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fcadcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 5.0\n"
     ]
    }
   ],
   "source": [
    "#Else\n",
    "\n",
    "try:\n",
    "    \n",
    "    result = 10 / 2\n",
    "    \n",
    "except ZeroDivisionError:\n",
    "    \n",
    "    print(\"Division by zero is not allowed.\")\n",
    "else:\n",
    "    \n",
    "    print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24659e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid conversion to integer.\n"
     ]
    }
   ],
   "source": [
    "#finally\n",
    "\n",
    "try:\n",
    "    \n",
    "    file = open(\"Assignment2.txt\", \"r\")\n",
    "    result = int(file.read())\n",
    "    \n",
    "except ValueError:\n",
    "   \n",
    "    print(\"Invalid conversion to integer.\")\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09bee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught an exception: This is a custom exception.\n"
     ]
    }
   ],
   "source": [
    "#Raising Exceptions:\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    raise ValueError(\"This is a custom exception.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    \n",
    "    print(f\"Caught an exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7759d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d40794",
   "metadata": {},
   "source": [
    "**4. What are list comprehensions in Python?**\n",
    "\n",
    "Ans: \n",
    "\n",
    "List comprehension in python is a concise way of creating lists from the ones that already exist. It provides a shorter syntax to create new lists from existing lists and their values.\n",
    "\n",
    "Syntax:\n",
    "    \n",
    "        listA = [expression for item in iterable if condition]\n",
    "        \n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bf6769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "#creating a list of squares\n",
    "\n",
    "squares_comp = [i**2 for i in range(5)]\n",
    "\n",
    "print(squares_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2583ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347a06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7156245d",
   "metadata": {},
   "source": [
    "**5.What is the purpose of the if __name__ == \"__main__\" statement?**\n",
    "\n",
    "Ans :\n",
    "    \n",
    " if __name__ == \"__main__\": statement in Python is used to check whether the Python script is being run as the main program or if it is being imported as a module into another script. \n",
    "This structure is commonly used to define a block of code that should only run when the script is executed directly, not when it is imported as a module.\n",
    "\n",
    " • When a Python script is executed, the special variable __name__ is set to \"__main__\" if the script is the top-level script being run.\n",
    "      \n",
    " •  If the script is being imported as a module into another script, the __name__ variable is set to             the name of the module (not \"__main__\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9655bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Code that should only run when the script is executed directly\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c55523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da31f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c961d9",
   "metadata": {},
   "source": [
    "**6.What is the purpose of the with statement in Python?**\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    In Python, with statement is used in exception handling to make the code cleaner and much more readable. It simplifies the management of common resources like file streams. Observe the following code example on how the use of with statement makes code cleaner. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5adf1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This File Contain demo of with statement\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "# Without using with statement\n",
    "file = open(\"demo.txt\", \"r\")\n",
    "data = file.read()\n",
    "print(data)\n",
    "file.close()  # Need to remember to close the file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b370c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This File Contain demo of with statement\n"
     ]
    }
   ],
   "source": [
    "# Using with statement\n",
    "with open(\"demo.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    print(data)\n",
    "# File is automatically closed, even if an exception occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddfff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b8a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be91eb3",
   "metadata": {},
   "source": [
    "**7.What are the key features of Spark?**\n",
    "\n",
    "Ans;\n",
    "\n",
    "Apache Spark is an open-source, distributed computing system that provides fast and general-purpose cluster-computing frameworks for big data processing. Here are some key features of Apache Spark:\n",
    "\n",
    "-    Speed:\n",
    "\n",
    "        Spark provides in-memory processing, which allows it to perform operations much faster than traditional disk-based systems. It can cache intermediate data in memory and reuse it across multiple stages of computation.\n",
    "\n",
    "\n",
    "-    Ease of Use:\n",
    "\n",
    "        Spark offers high-level APIs in Java, Scala, Python, and R, making it accessible to a wide range of developers with different programming language preferences. It also includes built-in libraries for SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n",
    "\n",
    "\n",
    "-    Distributed Data Processing:\n",
    "\n",
    "        Spark distributes data across a cluster and processes it in parallel. It automatically handles the distribution of data and computation across nodes, providing scalability.\n",
    "\n",
    "\n",
    "-    Fault Tolerance:\n",
    "\n",
    "        Spark provides fault tolerance through resilient distributed datasets (RDDs). If a partition of an RDD is lost due to node failure, Spark can reconstruct the lost data using lineage information and the transformations applied to the data.\n",
    "\n",
    "\n",
    "-    Lazy Evaluation:\n",
    "\n",
    "        Spark uses lazy evaluation, meaning that it postpones the execution of operations until an action is triggered. This allows it to optimize the execution plan and reduce unnecessary computations.\n",
    "\n",
    "\n",
    "-    Extensibility:\n",
    "\n",
    "        Spark is extensible, allowing developers to build custom extensions or use third-party libraries. It also supports integration with Hadoop Distributed File System (HDFS) and other Hadoop ecosystem tools.\n",
    "\n",
    "\n",
    "-    Real-time Stream Processing:\n",
    "\n",
    "        Spark Streaming allows processing and analyzing live data streams in real-time. It enables applications to ingest and process live data streams, making Spark suitable for both batch processing and stream processing workloads.\n",
    "\n",
    "\n",
    "-    Machine Learning Library (MLlib):\n",
    "\n",
    "        Spark includes a machine learning library (MLlib) that provides a variety of algorithms and tools for machine learning tasks such as classification, regression, clustering, and collaborative filtering.\n",
    "\n",
    "\n",
    "-    Graph Processing (GraphX):\n",
    "\n",
    "        Spark includes GraphX, a graph processing library that enables the analysis of graph-structured data. It provides a set of graph algorithms and a flexible graph computation API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b5ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37fdb045",
   "metadata": {},
   "source": [
    "**8.What are Resilient Distributed Datasets (RDDs) in Spark?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "RDD stands for “Resilient Distributed Dataset”. It is the fundamental data structure of Apache Spark. RDD in Apache Spark is an immutable collection of objects which computes on the different node of the cluster.\n",
    "\n",
    "Decomposing the name RDD:\n",
    "\n",
    "- Resilient, i.e. fault-tolerant with the help of RDD lineage graph(DAG) and so able to recompute missing or damaged partitions due to node failures.\n",
    "\n",
    "- Distributed, since Data resides on multiple nodes.\n",
    "\n",
    "- Dataset represents records of the data you work with. The user can load the data set externally which can be either JSON file, CSV file, text file or database via JDBC with no specific data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a83bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75efffdd",
   "metadata": {},
   "source": [
    "**9.What is the difference between a DataFrame and an RDD in Spark?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "Difference Between RDD and Dataframes\n",
    "\n",
    "RDD:\n",
    "\n",
    "- In Spark development, RDD refers to the distributed data elements collection across various devices in the cluster. It is a set of Scala or Java objects to represent data.\n",
    "\n",
    "- Spark RDD can easily process structured and unstructured data, but it does not provide the schema of added data and users need to identify it.\n",
    "\n",
    "- When RDD is integrated with the data sources API, it allows RDD to come from any source of data like a text file or a database. This way, without any predefined structure, data handling becomes easier.\n",
    "\n",
    "- RDD’s nature is immutable. It means that nothing is changeable in RDD but it can be created through various transformations. This way, the nature of all the calculations is consistent. This way, the nature of all the calculations is consistent.\n",
    "\n",
    "\n",
    " \n",
    "DataFrame:\n",
    "\n",
    "- Spark Dataframe refers to the distributed collection of organized data in named columns. It is like a relational database table.\n",
    "\n",
    "- A dataframe can process structured and semi-structured data only because it is like a relational database, and it can manage the schema.\n",
    "\n",
    "- When Dataframe is integrated with the data sources API, it allows the processing of data in various formats like JSON, HIVE tables, AVRO, MySQL, and CSV. This way, it can easily write and read from these data sources.\n",
    "\n",
    "- For Dataframe, a domain object cannot be regenerated after transformation. This way, if one test Dataframe is generated, the original RDD cannot be recovered again from the test class.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d23ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a14138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a89ea7d3",
   "metadata": {},
   "source": [
    "**10.What is Spark's ecosystem?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "The main Ecosystem components of the Spark Ecosystem are explained below:\n",
    "\n",
    "-    Spark Core:\n",
    "        The core engine and functionality of Apache Spark. It includes the basic APIs and features for distributed data processing, Resilient Distributed Datasets (RDDs), and Spark's core execution engine.\n",
    "\n",
    "\n",
    "-    Spark SQL:\n",
    "        Provides a programming interface for data manipulation using SQL queries. It allows Spark to seamlessly integrate with structured data using DataFrames and provides support for querying data stored in various formats, including Parquet, Avro, JSON, and more.\n",
    "\n",
    "\n",
    "-    Spark Streaming:\n",
    "        Enables processing and analyzing real-time data streams. Spark Streaming allows developers to apply batch processing operations to streaming data, making it suitable for near-real-time analytics.\n",
    "\n",
    "\n",
    "-    MLlib (Machine Learning Library):\n",
    "        A scalable machine learning library that provides a wide range of algorithms for classification, regression, clustering, and collaborative filtering. MLlib is designed for distributed computing and integrates with other Spark components.\n",
    "\n",
    "\n",
    "-    GraphX:\n",
    "        A graph processing library for graph-structured data. GraphX provides a distributed computation framework for graph analytics and supports a wide range of graph algorithms.\n",
    "\n",
    "\n",
    "-    SparkR:\n",
    "        Allows users to run Spark from the R programming language. SparkR provides an R package that enables R users to leverage the distributed computing capabilities of Spark for data analysis.\n",
    "\n",
    "\n",
    "-    PySpark:\n",
    "        A Python library for Apache Spark. PySpark allows Python developers to interact with Spark and perform data processing using Spark's distributed computing capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf65b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
